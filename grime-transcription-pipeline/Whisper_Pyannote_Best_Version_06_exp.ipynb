{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627882a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a99879",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install pyannote.audio\n",
    "!pip install torchaudio\n",
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f689d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìÅ Imports\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
  "token = \"XXXXXXXXXXXXXXXXXXXXXXXX\"\n",
  "#Remplacer\n",
  "\n",
  "# üìç Configuration\n",
  "audio_path = \"/content/drive/MyDrive/AudiosToTranscribe/Archive_001_LoganSamawithJME&NewhamGeneralsKissOct5th2010.mp3\"  # √† remplacer\n",
  "archive_title = \"LoganSamaNewhamGenerals\"\n",
  "mode_test = False\n",
  "whisper_model = \"large\"\n",
  "min_speakers = 2\n",
  "max_speakers = 4\n",
  "target_len = 7\n",
  "tolerance = 3\n",
  "pause_threshold = 0.8\n",
  "verbose = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üéß Conversion en WAV mono 16kHz\n",
    "audio = AudioSegment.from_mp3(audio_path)\n",
    "wav = audio.set_channels(1).set_frame_rate(16000)\n",
    "\n",
    "# üéØ Mode test : extrait de 1:15 √† 1:45 (30s)\n",
    "if mode_test:\n",
    "    start_ms = 75 * 1000\n",
    "    end_ms = 105 * 1000\n",
    "    clip = wav[start_ms:end_ms]\n",
    "    wav_path = \"test_clip.wav\"\n",
    "else:\n",
    "    wav_path = \"converted.wav\"\n",
    "    clip = wav\n",
    "\n",
    "clip.export(wav_path, format=\"wav\")\n",
    "print(f\"‚úÖ Fichier audio export√© : {wav_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìå Transcription avec Whisper\n",
  "model = whisper.load_model(whisper_model)\n",
  "result = model.transcribe(wav_path, word_timestamps=True)\n",
    "segments = result[\"segments\"]\n",
    "print(f\"‚úÖ {len(segments)} segments transcrits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d022e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üß† Diarisation\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=token)\n",
    "\n",
  "dz = pipeline(\n",
  "    {\"uri\": \"audio\", \"audio\": wav_path},\n",
  "    min_speakers=min_speakers,\n",
  "    max_speakers=max_speakers\n",
    ")\n",
    "print(\"‚úÖ Diarisation termin√©e\")\n",
    "\n",
    "# üéôÔ∏è Indexation par temps\n",
    "speaker_timeline = []\n",
    "for turn, _, speaker in dz.itertracks(yield_label=True):\n",
    "    speaker_timeline.append({\n",
    "        \"start\": turn.start,\n",
    "        \"end\": turn.end,\n",
    "        \"speaker\": speaker\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f885c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
  "# üß© Construction du JSON avec d√©coupage dynamique (~7 mots)\n",
    "final_data = {\n",
    "    \"title\": archive_title,\n",
    "    \"instrumentals\": [\n",
    "        {\"start\": 0.0, \"end\": 60.0, \"title\": \"PLACEHOLDER 1\", \"artist\": \"TODO\"},\n",
    "        {\"start\": 60.0, \"end\": 120.0, \"title\": \"PLACEHOLDER 2\", \"artist\": \"TODO\"},\n",
    "        {\"start\": 120.0, \"end\": 180.0, \"title\": \"PLACEHOLDER 3\", \"artist\": \"TODO\"}\n",
    "    ],\n",
    "    \"segments\": []\n",
    "}\n",
    "\n",
  "def find_speaker(start, end):\n",
  "    for s in speaker_timeline:\n",
  "        if s[\"start\"] <= start < s[\"end\"]:\n",
  "            return s[\"speaker\"]\n",
  "    return \"unknown\"\n",
  "\n",
  "def check_speaker_consistency(start, end):\n",
  "    active = [s for s in speaker_timeline if not (s['end'] <= start or s['start'] >= end)]\n",
  "    return len(set(a['speaker'] for a in active)) == 1\n",
    "\n",
  "def split_by_diction(words, target_len=7, tolerance=3, pause_threshold=0.8):\n",
  "    max_words = target_len + tolerance\n",
  "    min_words = max(1, target_len - tolerance)\n",
  "    punctuation = ('.', '!', '?', ',', ';', '‚Ä¶')\n",
  "    chunks = []\n",
  "    current = []\n",
  "    last_end = None\n",
  "    for word in words:\n",
  "        w = word[\"word\"].strip() if isinstance(word, dict) else word.word.strip()\n",
  "        start = word[\"start\"] if isinstance(word, dict) else word.start\n",
  "        if last_end is not None and start - last_end > pause_threshold:\n",
  "            if current:\n",
  "                chunks.append(current)\n",
  "                current = []\n",
  "        current.append(word)\n",
  "        last_end = word[\"end\"] if isinstance(word, dict) else word.end\n",
  "        if len(current) >= max_words or w.endswith(punctuation):\n",
  "            chunks.append(current)\n",
  "            current = []\n",
  "    if current:\n",
  "        if len(current) < min_words and chunks:\n",
  "            chunks[-1].extend(current)\n",
  "        else:\n",
  "            chunks.append(current)\n",
  "    return chunks\n",
    "\n",
    "segment_id = 0\n",
    "for seg in segments:\n",
    "    words = seg[\"words\"]\n",
    "    for word in words:\n",
    "        word.pop(\"seek\", None)\n",
  "    chunks = split_by_diction(words, target_len=target_len, tolerance=tolerance, pause_threshold=pause_threshold)\n",
    "    for chunk in chunks:\n",
  "        start_time = chunk[0][\"start\"]\n",
  "        end_time = chunk[-1][\"end\"]\n",
  "        spk = find_speaker(start_time, end_time)\n",
  "        segment = {\n",
  "            \"id\": segment_id,\n",
  "            \"start\": start_time,\n",
  "            \"end\": end_time,\n",
  "            \"text\": ' '.join([w[\"word\"] for w in chunk]),\n",
  "            \"words\": chunk,\n",
  "            \"speaker\": spk,\n",
  "            \"instrumental\": None\n",
  "        }\n",
  "        final_data[\"segments\"].append(segment)\n",
  "        if verbose and (len(chunk) > target_len + tolerance or len(chunk) < max(1, target_len - tolerance)):\n",
  "            print(f'‚ö†Ô∏è segment {segment_id} longueur inhabituelle: {len(chunk)} mots')\n",
  "        if verbose and not check_speaker_consistency(start_time, end_time):\n",
  "            print(f'‚ö†Ô∏è segment {segment_id} contient plusieurs locuteurs')\n",
  "        segment_id += 1\n",
    "\n",
    "# Export\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"archive_output_{timestamp}.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(final_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Fichier export√© : {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
