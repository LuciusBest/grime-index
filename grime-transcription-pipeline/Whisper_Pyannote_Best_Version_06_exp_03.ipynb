{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627882a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a99879",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install pyannote.audio\n",
    "!pip install torchaudio\n",
    "!pip install pydub\n",
    "!pip install yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f689d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìÅ Imports\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import Video, Audio\n",
    "import subprocess, json, os\n",
    "from datetime import datetime\n",
    "\n",
    "token = \"XXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "#Remplacer\n",
    "\n",
    "# üìç Configuration\n",
    "youtube_url = \"https://www.youtube.com/watch?v=XXXXXXXXXXX\"  # √† remplacer\n",
    "start_time = 0\n",
    "end_time = 60\n",
    "archive_title = \"ARCHIVE_008\"\n",
    "whisper_model = \"large\"\n",
    "min_speakers = 2\n",
    "max_speakers = 4\n",
    "target_len = 7\n",
    "tolerance = 3\n",
    "pause_threshold = 0.8\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75cb2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîç Download and clip video\n",
    "video_path = \"downloaded_video.mp4\"\n",
    "subprocess.run([\"yt-dlp\", \"-o\", video_path, youtube_url], check=True)\n",
    "clipped_video = \"clip_video.mp4\"\n",
    "duration = float(end_time) - float(start_time)\n",
    "subprocess.run([\"ffmpeg\", \"-y\", \"-ss\", str(start_time), \"-i\", video_path, \"-t\", str(duration), \"-c\", \"copy\", clipped_video], check=True)\n",
    "wav_path = \"clip_audio.wav\"\n",
    "subprocess.run([\"ffmpeg\", \"-y\", \"-ss\", str(start_time), \"-i\", video_path, \"-t\", str(duration), \"-ac\", \"1\", \"-ar\", \"16000\", \"-vn\", wav_path], check=True)\n",
    "os.remove(video_path)\n",
    "display(Video(clipped_video))\n",
    "display(Audio(wav_path))\n",
    "print(f\"‚úÖ Video and audio prepared: {clipped_video}, {wav_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìå Transcription avec Whisper\n",
    "model = whisper.load_model(whisper_model)\n",
    "result = model.transcribe(wav_path, word_timestamps=True)\n",
    "segments = result[\"segments\"]\n",
    "print(f\"‚úÖ {len(segments)} segments transcrits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d022e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üß† Diarisation\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=token)\n",
    "\n",
    "dz = pipeline(\n",
    "    {\"uri\": \"audio\", \"audio\": wav_path},\n",
    "    min_speakers=min_speakers,\n",
    "    max_speakers=max_speakers\n",
    ")\n",
    "print(\"‚úÖ Diarisation termin√©e\")\n",
    "\n",
    "# üéôÔ∏è Indexation par temps\n",
    "speaker_timeline = []\n",
    "for turn, _, speaker in dz.itertracks(yield_label=True):\n",
    "    speaker_timeline.append({\n",
    "        \"start\": turn.start,\n",
    "        \"end\": turn.end,\n",
    "        \"speaker\": speaker\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f885c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üß© Construction du JSON avec d√©coupage dynamique (~7 mots)\n",
    "final_data = {\n",
    "    \"title\": archive_title,\n",
    "    \"instrumentals\": [\n",
    "        {\"start\": 0.0, \"end\": 60.0, \"title\": \"PLACEHOLDER 1\", \"artist\": \"TODO\"},\n",
    "        {\"start\": 60.0, \"end\": 120.0, \"title\": \"PLACEHOLDER 2\", \"artist\": \"TODO\"},\n",
    "        {\"start\": 120.0, \"end\": 180.0, \"title\": \"PLACEHOLDER 3\", \"artist\": \"TODO\"}\n",
    "    ],\n",
    "    \"reloads\": [],\n",
    "    \"segments\": [],\n",
    "    \"overlaps\": {\"manual\": [], \"auto\": []}\n",
    "}\n",
    "\n",
    "def find_speaker(start, end):\n",
    "    for s in speaker_timeline:\n",
    "        if s[\"start\"] <= start < s[\"end\"]:\n",
    "            return s[\"speaker\"]\n",
    "    return \"unknown\"\n",
    "\n",
    "def check_speaker_consistency(start, end):\n",
    "    active = [s for s in speaker_timeline if not (s['end'] <= start or s['start'] >= end)]\n",
    "    return len(set(a['speaker'] for a in active)) == 1\n",
    "\n",
    "def split_by_diction(words, target_len=7, tolerance=3, pause_threshold=0.8):\n",
    "    max_words = target_len + tolerance\n",
    "    min_words = max(1, target_len - tolerance)\n",
    "    punctuation = ('.', '!', '?', ',', ';', '‚Ä¶')\n",
    "    chunks = []\n",
    "    current = []\n",
    "    last_end = None\n",
    "    for word in words:\n",
    "        w = word[\"word\"].strip() if isinstance(word, dict) else word.word.strip()\n",
    "        start = word[\"start\"] if isinstance(word, dict) else word.start\n",
    "        if last_end is not None and start - last_end > pause_threshold:\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "                current = []\n",
    "        current.append(word)\n",
    "        last_end = word[\"end\"] if isinstance(word, dict) else word.end\n",
    "        if len(current) >= max_words or w.endswith(punctuation):\n",
    "            chunks.append(current)\n",
    "            current = []\n",
    "    if current:\n",
    "        if len(current) < min_words and chunks:\n",
    "            chunks[-1].extend(current)\n",
    "        else:\n",
    "            chunks.append(current)\n",
    "    return chunks\n",
    "\n",
    "segment_id = 0\n",
    "for seg in segments:\n",
    "    words = seg[\"words\"]\n",
    "    for word in words:\n",
    "        word.pop(\"seek\", None)\n",
    "        word[\"chorus\"] = False\n",
    "    chunks = split_by_diction(words, target_len=target_len, tolerance=tolerance, pause_threshold=pause_threshold)\n",
    "    for chunk in chunks:\n",
    "        start_time = chunk[0][\"start\"]\n",
    "        end_time = chunk[-1][\"end\"]\n",
    "        spk = find_speaker(start_time, end_time)\n",
    "        segment = {\n",
    "            \"id\": segment_id,\n",
    "            \"start\": start_time,\n",
    "            \"end\": end_time,\n",
    "            \"text\": ' '.join([w[\"word\"] for w in chunk]),\n",
    "            \"words\": chunk,\n",
    "            \"speaker\": spk,\n",
    "            \"instrumental\": None\n",
    "        }\n",
    "        final_data[\"segments\"].append(segment)\n",
    "        single = check_speaker_consistency(start_time, end_time)\n",
    "        if verbose and (len(chunk) > target_len + tolerance or len(chunk) < max(1, target_len - tolerance)):\n",
    "            print(f'‚ö†Ô∏è segment {segment_id} longueur inhabituelle: {len(chunk)} mots')\n",
    "        if verbose and not single:\n",
    "            print(f'‚ö†Ô∏è segment {segment_id} contient plusieurs locuteurs')\n",
    "        if not single:\n",
    "            final_data[\"overlaps\"][\"auto\"].append(segment.copy())\n",
    "        segment_id += 1\n",
    "\n",
    "# Export\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"archive_output_{timestamp}.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(final_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Fichier export√© : {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
